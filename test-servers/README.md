# Test Server Environment for Distributed Log Analysis Framework

This directory contains a complete test environment for the Distributed Log Analysis Framework. It's designed to generate logs, process them, validate consistency, and monitor system performance through metrics visualization.

## Architecture Overview

The test environment consists of several interconnected components:

![Architecture Diagram](https://www.mermaidchart.com/raw/950bf1f1-2eb2-46eb-89c1-577f91c8b0bd?theme=light&version=v0.1&format=svg)

## Components

### Python Server (`server/`)

A log generator service that simulates web server activity by creating and sending logs to RabbitMQ.

- **Key Features**:
  - Generates realistic HTTP server logs with random IPs, endpoints, HTTP methods, etc.
  - Uses multi-threading for high-volume log generation
  - Exports Prometheus metrics about generation rate
  - Configurable through environment variables

- **Environment Variables**:
  - `RABBITMQ_HOST`, `RABBITMQ_PORT`, `RABBITMQ_USER`, `RABBITMQ_PASSWORD` - RabbitMQ connection settings
  - `RABBITMQ_QUEUE` - Queue name for logs
  - `LOG_INTERVAL` - Delay between log batches (seconds)
  - `BATCH_SIZE` - Number of logs in each batch
  - `NUM_THREADS` - Number of generator threads
  - `SERVER_PORT` - Port for metrics exposure

### RabbitMQ

Message queue that acts as a buffer between the log generator and the log processor.

- **Configuration**:
  - Default credentials (guest/guest)
  - Management UI available on port 15672
  - Durable queue configuration for reliability

### Consistency Validator (`consistency_validator/`)

A service that monitors the entire pipeline to ensure logs are processed correctly.

- **Key Features**:
  - Compares generated logs count with processed logs count
  - Accounts for logs in queue awaiting processing
  - Calculates and exports consistency metrics
  - Detects potential data loss or duplicate processing
  - Analyzes trends to determine if issues are improving or worsening
  - Exports detailed metrics to Prometheus

- **Environment Variables**:
  - `MONGO_URI`, `MONGO_DATABASE`, `MONGO_COLLECTION` - MongoDB connection settings
  - `CHECK_INTERVAL` - Time between consistency checks (seconds)
  - `PYTHON_SERVER_METRICS_URL` - URL to fetch generator metrics
  - `RABBITMQ_*` - RabbitMQ connection settings
  - `CONSISTENCY_THRESHOLD_LOW`, `CONSISTENCY_THRESHOLD_HIGH` - Thresholds for consistency alerts
  - `METRICS_PORT` - Port for Prometheus metrics

### Prometheus (`prometheus/`)

Time-series database for storing and querying metrics from all components.

- **Configuration**:
  - Scrapes metrics from Python Server and Consistency Validator
  - Default settings: 15s scrape interval, 15s evaluation interval
  - Configured via `prometheus.yml`

### Grafana (`grafana/`)

Visualization platform for metrics stored in Prometheus.

- **Dashboard Features**:
  - Real-time log generation vs. processing rates
  - Consistency ratio gauge (ideal: 95-105%)
  - RabbitMQ queue depth monitoring
  - Estimated processing time for queued messages
  - Consistency trends over time
  - Error detection and visualization

## Metrics Overview

The system exposes the following key metrics:

| Metric | Type | Description |
|--------|------|-------------|
| `logs_generated_total` | Counter | Total number of logs generated by Python Server |
| `logs_sent_total` | Counter | Total number of logs sent to RabbitMQ |
| `logs_processed_total` | Gauge | Total number of logs processed and stored in MongoDB |
| `rabbitmq_queue_depth` | Gauge | Current number of messages in the RabbitMQ queue |
| `consistency_ratio` | Gauge | Ratio between processed and generated logs (percentage) |
| `estimated_processing_time_seconds` | Gauge | Estimated time to process all queued logs |
| `consistency_checks_total` | Counter | Total number of consistency checks performed |
| `consistency_errors_total` | Counter | Total number of consistency errors detected |
| `connection_errors_total` | Counter | Total connection errors during log generation/processing |
| `active_workers` | Gauge | Number of active worker threads in Python Server |

## Grafana Dashboard

The provided dashboard (`grafana/dashboard.json`) visualizes system performance with:

1. **Logs (Generated vs Processed)**: Time-series graph showing the gap between logs generated and processed
2. **Data Consistency Ratio**: Gauge showing processing consistency percentage
3. **Log Counters**: Current totals for generated and processed logs
4. **RabbitMQ Queue Depth**: Current number of messages in queue
5. **Queue Depth Trend**: Time-series graph of queue depth over time
6. **Estimated Queue Processing Time**: Time required to process the current queue
7. **Consistency Ratio Trend**: Time-series graph showing consistency patterns
8. **Consistency Errors**: Count of detected consistency problems over time

The dashboard uses color-coded thresholds:
- Green: System functioning normally (consistency 95-105%)
- Yellow: Warning range (consistency 80-95% or 105-120%)
- Red: Critical issues (consistency <80% or >120%)

### Detailed Dashboard Interpretation

#### Generated vs Processed Graph Convergence

The "Logs (Generated vs Processed)" graph is a critical visualization of our system's data flow integrity. The two lines typically follow similar trajectories but with important characteristics:

1. **Convergence Pattern**: In a healthy system, both lines should grow at similar rates, with the "Generated" line slightly leading the "Processed" line. This convergence pattern indicates that logs are being processed at a rate comparable to their generation, allowing for the expected processing delay.

2. **Gap Interpretation**: A small gap between the lines represents logs in transit (either in the RabbitMQ queue or being processed). This gap should remain relatively constant in a stable system. If the gap widens over time, it suggests that processing is falling behind generation.

#### Processed Graph Stepped Appearance

The "Processed" line typically shows a stepped pattern rather than a smooth curve due to the metric update frequency. The processed metrics are updated at specific intervals (15 seconds) determined by the batch completion, while generated metrics may update more frequently.

This stepped appearance is normal and expected behavior, which helps to observe metrics in multithreading environment.

#### Data Consistency Ratio Boundaries

The Data Consistency Ratio gauge uses specific thresholds that have been carefully calibrated to reflect the operational realities of the distributed system:

1. **Optimal Range (95-105%, Green)**:
   - A ratio between 95-105% indicates healthy system operation
   - The 5% tolerance accounts for normal processing delays and measurement timing differences
   - In this range, the system is considered fully reliable

2. **Warning Range (80-95% or 105-120%, Yellow)**:
   - Lower range (80-95%): Some logs may be delayed in processing or potentially lost
   - Upper range (105-120%): Possible duplicate processing or measurement anomalies
   - Investigation is recommended but not critical

3. **Critical Range (<80% or >120%, Red)**:
   - Below 80%: Significant log loss or severe processing delays
   - Above 120%: Serious duplication issues or fundamental measurement errors
   - Immediate investigation is necessary

#### Log Counter Difference Interpretation

The "Log Counters" panel often shows a slight difference between the "Generated" and "Processed" counts, with Generated typically higher than Processed. This is both expected and correct for several reasons:

1. **Pipeline Latency**: Logs take time to flow through the system from generation to processing completion. At any given moment, some logs are in transit (in RabbitMQ or being processed).

2. **Measurement Timing**: The metrics are collected at slightly different points in time due to the Prometheus scrape interval, creating small natural discrepancies.

3. **Asynchronous Processing**: The log generation and processing systems operate asynchronously and independently, so perfect count alignment would actually be suspicious.

A small, stable difference (typically proportional to the generation rate and system scale) indicates the system is functioning correctly. Only a growing difference over time should be cause for concern, as it indicates the processing cannot keep up with generation.

## Running the Environment

To start the entire test environment:

```bash
cd test-servers
docker-compose up -d
```

Service endpoints:
- Grafana: http://localhost:3000 (admin/admin)
- Prometheus: http://localhost:9090 (http://prometheus:9090 for Grafana Datasource)
- RabbitMQ Management: http://localhost:15672 (guest/guest)
- MongoDB Express: http://localhost:8082 (admin/pass)
